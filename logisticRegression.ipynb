{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00dda75",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression for Injury Prediction\n",
    "\n",
    "**What is Logistic Regression?**  \n",
    "Logistic regression is a method for modeling binary outcomes (e.g., injury yes/no). It estimates the probability that an input belongs to the positive class.\n",
    "\n",
    "**Why use it?**  \n",
    "- For binary classification tasks.  \n",
    "- Provides interpretable coefficients indicating the impact of features on injury risk.  \n",
    "- Allows comparison of categorical predictors relative to a baseline category.\n",
    "\n",
    "**Outputs:**  \n",
    "- Predicted probabilities for target variable.  \n",
    "- Predicted class labels of target variable (0 or 1).  \n",
    "- Model coefficients showing feature impact on target variable.\n",
    "\n",
    "**Why analyze position effects on injuries?**  \n",
    "Player position is a fundamental aspect of soccer tactics. Different positions involve different running loads, collision frequencies, and playing styles. Knowing which positions carry higher or lower injury risks helps coaches and medical staff:\n",
    "1. **Target prevention**: Tailor conditioning or recovery programs by position.  \n",
    "2. **Squad planning**: Manage rotations to minimize risk for vulnerable roles.  \n",
    "3. **Data-driven decisions**: Allocate resources (e.g., physiotherapists) where they have highest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7febf",
   "metadata": {},
   "source": [
    "## 1. Imports and Data Loading\n",
    "\n",
    "Import necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "975bf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing    import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.linear_model     import LogisticRegression\n",
    "from imblearn.over_sampling   import SMOTE\n",
    "from sklearn.metrics          import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    roc_auc_score, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e293ac",
   "metadata": {},
   "source": [
    "## 2. Build the Combined Dataset\n",
    "\n",
    "Load your datasets, and merge the important tables into a single DataFrame for easier feature engineering and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e0aee434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this data import with your real data.\n",
    "df_player_stats = pd.read_csv('player_stats_table.csv')\n",
    "df_player       = pd.read_csv('player_table.csv')\n",
    "df_match        = pd.read_csv('match_table.csv')\n",
    "\n",
    "df = (\n",
    "    df_player_stats\n",
    "      .merge(df_player, on='player_id', how='left')\n",
    "      .merge(df_match,  on='match_id',  how='left')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09075da3",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create new features that are not directly available in the raw data.  \n",
    "Example: Calculate player age in years from their birthdate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "65cbadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age\n",
      "0   35\n",
      "1   24\n",
      "2   29\n",
      "3   26\n",
      "4   39\n"
     ]
    }
   ],
   "source": [
    "df['age'] = (pd.Timestamp('today') - pd.to_datetime(df['birthdate'])).dt.days // 365\n",
    "\n",
    "# Test if the values have been computed correctly.\n",
    "print(df[['age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70c857",
   "metadata": {},
   "source": [
    "## 4. Select and Split Features and Target\n",
    "\n",
    "Choose the predictor variables and the target variable.\n",
    "\n",
    "Create a separate copy of the selected columns to avoid modifying the original DataFrame `df`. This is important when performing transformations (e.g., scaling) that shouldn't affect `df`.\n",
    "\n",
    "Finally, separate features (X) and target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "3c74c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'model_columns' to include whatever predictors you need.\n",
    "model_columns = [\n",
    "    'position',\n",
    "    'age',\n",
    "    'minutes_played',\n",
    "    'distance_covered_in_meters',\n",
    "    'sprints',\n",
    "    'rest_days_since_last_match',\n",
    "    'is_back_to_back_away_game',\n",
    "    'injury_occurred'   # target: 1=inured, 0=healthy.\n",
    "]\n",
    "\n",
    "model_df = df[model_columns].copy()\n",
    "\n",
    "X = model_df.drop(columns='injury_occurred')\n",
    "y = model_df['injury_occurred']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc7cc1",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split\n",
    "\n",
    "Split data into training and testing sets to train the model on one portion and evaluate its performance on unseen data. Stratification is used to keep class balance.\n",
    "\n",
    "Stratification ensures that the proportion of injury cases (positive vs. negative classes) is maintained in both the training and test sets (class balance). This is especially important for imbalanced classification problems, where one class may dominate. Stratification prevents the test set from being unrepresentative.\n",
    "\n",
    "You can adjust 'test_size' to control how much data goes into the test set. Common values range from 0.2 to 0.3 (i.e., 20–30% of the data used for testing).\n",
    "The 'random_state' ensures reproducibility: using the same value will yield the same split every time. You can choose any integer value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "7346dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e2f3f",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Features\n",
    "\n",
    "Many machine learning models cannot handle categorical variables directly.\n",
    "One-Hot-Encoding transforms a categorical feature (e.g., 'position') into multiple binary (0/1) features — one for each category, indicating whether a sample belongs to that category.\n",
    "This allows the model to learn a separate coefficient for each category.\n",
    "\n",
    "When One-Hot-Encoding is applied, one column is dropped and becomes the baseline. All coefficients of the remaining categories will be interpreted in relation to this dropped base category.\n",
    "\n",
    "Repeat the following steps for every categorical column you have among your predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "74c6cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All positions in the dataset:\n",
      "['Defender' 'Forward' 'Goalkeeper' 'Midfielder']\n",
      "\n",
      "Reference (baseline) position used for comparison: Goalkeeper\n"
     ]
    }
   ],
   "source": [
    "# Show all categories to decide which to drop.\n",
    "print(\"All positions in the dataset:\")\n",
    "print(np.sort(df['position'].unique()))\n",
    "\n",
    "# Apply One-Hot-Encoding to the 'position' column.\n",
    "# Choose in the 'drop' argument which category you want to use as the reference category.\n",
    "encoder = OneHotEncoder(drop=['Goalkeeper'], sparse_output=False)\n",
    "encoder.fit(X_train[['position']])\n",
    "pos_cols = encoder.get_feature_names_out(['position'])\n",
    "X_train_pos = pd.DataFrame(\n",
    "    encoder.transform(X_train[['position']]),\n",
    "    columns=pos_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_pos = pd.DataFrame(\n",
    "    encoder.transform(X_test[['position']]),\n",
    "    columns=pos_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Show which category was dropped (= baseline) for interpretation later.\n",
    "reference_position = list(set(df['position'].unique()) - set([col.split(\"_\")[1] for col in pos_cols]))\n",
    "print(f\"\\nReference (baseline) position used for comparison: {reference_position[0]}\")\n",
    "\n",
    "# Remove original 'position' column and append the dummies that have just been created.\n",
    "X_train = pd.concat([X_train.drop(columns='position'), X_train_pos], axis=1)\n",
    "X_test  = pd.concat([X_test.drop(columns='position'),  X_test_pos],  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e900e04",
   "metadata": {},
   "source": [
    "## 7. Scale Numeric Features\n",
    "\n",
    "Standardize numeric columns so they have mean = 0 and standard deviation = 1. This is important because many machine learning models are sensitive to feature scales. Features with larger ranges can dominate the model's learning process.\n",
    "Standardization ensures that each feature contributes equally to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "e8f1d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'age',\n",
    "    'minutes_played',\n",
    "    'distance_covered_in_meters',\n",
    "    'sprints',\n",
    "    'rest_days_since_last_match'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numeric_cols])\n",
    "X_train[numeric_cols] = scaler.transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols]  = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188bf8d",
   "metadata": {},
   "source": [
    "## 8. Oversampling\n",
    "\n",
    "Generate synthetic examples for the smaller class to generate more balanced classes.  \n",
    "You can change the value for 'k_neighbors' to see how it influences the evaluation metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "a6c65755",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42, k_neighbors=30)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96f547",
   "metadata": {},
   "source": [
    "## 9. Train Logistic Regression Model\n",
    "\n",
    "Train logistic regression on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "9f2f2311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {color: black;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', solver='liblinear')"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6adf54",
   "metadata": {},
   "source": [
    "## 10. Make Predictions and Evaluate Performance\n",
    "\n",
    "Predict the class of the target variable and probability on the test set.  \n",
    "We compute both y_pred and y_pred_prob because:\n",
    "- y_pred gives the final binary classification (\"Injury YES (1) or NO (0)\"), used for metrics like accuracy, precision, recall.\n",
    "- y_pred_prob gives the predicted probability for the positive class (injury=true), used for metrics like ROC AUC.  \n",
    "\n",
    "- Accuracy: overall correct predictions\n",
    "- Precision: how many predicted positives are actually true - important when false positives are costly (e.g. falsely predicting an injury).\n",
    "- Recall: how many true positives were detected - important when missing a positive case is costly (e.g. overlooking a real injury).\n",
    "- ROC AUC: measures how well the model ranks positives higher than negatives - a value close to 1.0 means the model separates classes well; 0.5 means random guessing.\n",
    "\n",
    "Having both allows us to evaluate the model performance comprehensively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "bdec726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.57\n",
      "Precision: 0.33\n",
      "Recall   : 0.52\n",
      "ROC AUC  : 0.56\n"
     ]
    }
   ],
   "source": [
    "y_pred      = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Recall   : {recall_score(y_test, y_pred):.2f}\")\n",
    "print(f\"ROC AUC  : {roc_auc_score(y_test, y_pred_prob):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab3c88",
   "metadata": {},
   "source": [
    "# 11. Interpret Model Coefficients\n",
    "\n",
    "Coefficients show how each feature affects the occurrence of the target variable (here: injury occurrence) in terms of log-odds.  \n",
    "\n",
    "- Numeric features (e.g., 'sprints', 'age') indicate the change in injury log-odds per unit increase. Example: A coefficient of 0.3 for 'sprints' means that each additional sprint slightly increases the risk.\n",
    "- Categorical features (like 'positions') are interpreted relative to the baseline category ('Goalkeeper'). A negative coefficient means a lower injury risk compared to the reference, while a positive one means a higher risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "1b76e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature impacts on injury risk (log-odds):\n",
      "                      feature  coefficient\n",
      "6           position_Defender     0.583332\n",
      "7            position_Forward     0.430203\n",
      "3                     sprints     0.296763\n",
      "4  rest_days_since_last_match     0.104831\n",
      "5   is_back_to_back_away_game     0.034987\n",
      "1              minutes_played    -0.021885\n",
      "8         position_Midfielder    -0.215823\n",
      "0                         age    -0.235150\n",
      "2  distance_covered_in_meters    -0.241319\n"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'feature':     X_train.columns,\n",
    "    'coefficient': model.coef_[0]\n",
    "}).sort_values(by='coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature impacts on injury risk (log-odds):\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37290955",
   "metadata": {},
   "source": [
    "## 12. Filter the Results\n",
    "\n",
    "Filter only the 'position' coefficients to see the influence of the position on the occurrence of injuries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "7169e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Impact of player position on injury risk:\n",
      "               feature  coefficient\n",
      "6    position_Defender     0.583332\n",
      "7     position_Forward     0.430203\n",
      "8  position_Midfielder    -0.215823\n"
     ]
    }
   ],
   "source": [
    "position_coef_df = coef_df[coef_df['feature'].str.startswith('position_')]\n",
    "print(\"\\nImpact of player position on injury risk:\")\n",
    "print(position_coef_df.sort_values(by='coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f66bc7",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "- Defenders are significantly more likely to be injured than goalkeepers.\n",
    "- Forwards are also more likely to be injured than goalkeepers, but less than defenders.\n",
    "- Midfielders are slightly less likely to be injured than goalkeepers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
