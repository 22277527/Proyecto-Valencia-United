{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00dda75",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression for Injury Prediction\n",
    "\n",
    "**What is Logistic Regression?**  \n",
    "Logistic regression is a method for modeling binary outcomes (e.g., injury yes/no). It estimates the **probability** that an input belongs to the positive class.\n",
    "\n",
    "**Why use it?**  \n",
    "- For binary classification tasks.  \n",
    "- Provides interpretable coefficients indicating the impact of features on injury risk.  \n",
    "- Allows comparison of categorical predictors relative to a baseline category.\n",
    "\n",
    "**Outputs:**  \n",
    "- Predicted probabilities for injury occurrence.  \n",
    "- Predicted injury class labels (0 or 1).  \n",
    "- Model coefficients showing feature impact on injury risk.\n",
    "\n",
    "**Why analyze position effects on injuries?**  \n",
    "Player position is a fundamental aspect of soccer tactics. Different positions involve different running loads, collision frequencies, and playing styles. Knowing which positions carry higher or lower injury risks helps coaches and medical staff:\n",
    "1. **Target prevention**: Tailor conditioning or recovery programs by position.  \n",
    "2. **Squad planning**: Manage rotations to minimize risk for vulnerable roles.  \n",
    "3. **Data-driven decisions**: Allocate resources (e.g., physiotherapists) where they have highest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7febf",
   "metadata": {},
   "source": [
    "## 1. Imports and Data Loading\n",
    "\n",
    "Import necessary Python libraries and load your datasets.  \n",
    "**Note:** Replace the example data import with your real data source (e.g., CSV files or database queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975bf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing    import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.linear_model     import LogisticRegression\n",
    "from imblearn.over_sampling   import SMOTE\n",
    "from sklearn.metrics          import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "# Replace this with your real data loading:\n",
    "# from FakeGenerationData import df_player_stats, df_player, df_match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e293ac",
   "metadata": {},
   "source": [
    "## 2. Build the Combined Dataset\n",
    "\n",
    "Merge player stats with player information and match data into a single DataFrame for easier feature engineering and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0aee434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your sources are CSVs:\n",
    "df_player_stats = pd.read_csv('player_stats_table.csv')\n",
    "df_player       = pd.read_csv('player_table.csv')\n",
    "df_match        = pd.read_csv('match_table.csv')\n",
    "\n",
    "df = (\n",
    "    df_player_stats\n",
    "      .merge(df_player, on='player_id', how='left')\n",
    "      .merge(df_match,  on='match_id',  how='left')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09075da3",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create new features that are not directly available in the raw data.  \n",
    "Example: Calculate player age in years from their birthdate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cbadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = (pd.Timestamp('today') - pd.to_datetime(df['birthdate'])).dt.days // 365"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70c857",
   "metadata": {},
   "source": [
    "## 4. Select Features and Target\n",
    "\n",
    "Choose the predictor variables and the target variable (injury occurrence).  \n",
    "Make a copy of these columns to keep transformations safe.\n",
    "\n",
    "Create a separate copy of the selected columns to avoid modifying the original DataFrame `df`.  \n",
    "This is important when performing transformations (e.g., scaling) that shouldn't affect `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c74c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'model_columns' to include whatever predictors you need.\n",
    "model_columns = [\n",
    "    'position',\n",
    "    'age',\n",
    "    'minutes_played',\n",
    "    'distance_covered_in_meters',\n",
    "    'sprints',\n",
    "    'rest_days_since_last_match',\n",
    "    'is_back_to_back_away_game',\n",
    "    'injury_occurred'   # target: 1=inured, 0=healthy\n",
    "]\n",
    "\n",
    "model_df = df[model_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea96623",
   "metadata": {},
   "source": [
    "## 5. Split Features and Target\n",
    "\n",
    "Separate features (X) and target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384050a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop(columns='injury_occurred')\n",
    "y = model_df['injury_occurred']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e2f3f",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Features\n",
    "\n",
    "Many machine learning models cannot handle categorical variables directly.\n",
    "One-Hot-Encoding transforms a categorical feature (e.g., \"position\") into multiple binary (0/1) features — one for each category, indicating whether a sample belongs to that category.\n",
    "This allows the model to learn a separate coefficient for each category.\n",
    "\n",
    "When One-Hot-Encoding is applied, the first column is dropped and becomes the baseline. All coefficients of the remaining categories will be interpreted in relation to this dropped base category.\n",
    "\n",
    "Repeat the following steps for every categorical column you have among your predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c6cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All player positions in the dataset:\n",
      "['Defender' 'Forward' 'Goalkeeper' 'Midfielder']\n",
      "\n",
      "Reference (baseline) position used for comparison: Defender\n"
     ]
    }
   ],
   "source": [
    "# List all unique positions (alphabetically sorted) in the dataset to get an overview of all categories\n",
    "# and to know which will be dropped (the first).\n",
    "print(\"All player positions in the dataset:\")\n",
    "print(np.sort(df['position'].unique()))\n",
    "\n",
    "# Apply One-Hot-Encoding to the \"position\" column.\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "pos_encoded = encoder.fit_transform(X[['position']])\n",
    "pos_cols    = encoder.get_feature_names_out(['position'])\n",
    "df_pos      = pd.DataFrame(pos_encoded, columns=pos_cols, index=X.index)\n",
    "\n",
    "# Show which category was dropped (baseline) for interpretation later.\n",
    "reference_position = list(set(df['position'].unique()) - set([col.split(\"_\")[1] for col in pos_cols]))\n",
    "print(f\"\\nReference (baseline) position used for comparison: {reference_position[0]}\")\n",
    "\n",
    "# Remove original 'position' column and append the dummies that have just been created:\n",
    "X = pd.concat([df_pos, X.drop(columns='position')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc7cc1",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split\n",
    "\n",
    "Split data into training and testing sets with stratification to keep class balance.\n",
    "\n",
    "Stratification ensures that the proportion of injury cases (positive vs. negative classes) is maintained in both the training and test sets (class balance). This is especially important for imbalanced classification problems, where one class may dominate. Stratification prevents the test set from being unrepresentative.\n",
    "\n",
    "You can adjust 'test_size' to control how much data goes into the test set. Common values range from 0.2 to 0.3 (i.e., 20–30% of the data used for testing).\n",
    "The 'random_state' ensures reproducibility: using the same value will yield the same split every time. You can choose any integer value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7346dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e900e04",
   "metadata": {},
   "source": [
    "## 8. Scale Numeric Features\n",
    "\n",
    "Standardize numeric columns so they have mean=0 and standard deviation=1. This is important because many machine learning models are sensitive to feature scales. Features with larger ranges can dominate the model's learning process.\n",
    "Standardization ensures that each feature contributes equally to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f1d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'age',\n",
    "    'minutes_played',\n",
    "    'distance_covered_in_meters',\n",
    "    'sprints',\n",
    "    'rest_days_since_last_match'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols]  = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188bf8d",
   "metadata": {},
   "source": [
    "## 9. Oversampling\n",
    "\n",
    "Generate synthetic examples for the smaller class to generate more balanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c65755",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42, k_neighbors=2)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96f547",
   "metadata": {},
   "source": [
    "## 10. Train Logistic Regression Model\n",
    "\n",
    "Train logistic regression on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2f2311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', solver='liblinear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6adf54",
   "metadata": {},
   "source": [
    "## 11. Make Predictions and Probabilities\n",
    "\n",
    "Predict injury class and probability on the test set.  \n",
    "We compute both y_pred and y_pred_prob because:\n",
    "- y_pred gives the final binary classification (\"Injury YES (1) or NO (0)\"), used for metrics like accuracy, precision, recall.\n",
    "- y_pred_prob gives the predicted probability for the positive class (injury=true), used for metrics like ROC AUC.  \n",
    "\n",
    "Having both allows us to evaluate the model performance comprehensively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced6b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred      = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df58199",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model Performance\n",
    "\n",
    "- Accuracy: overall correct predictions\n",
    "- Precision: how many predicted positives are actually true - important when false positives are costly (e.g. falsely predicting an injury).\n",
    "- Recall: how many true positives were detected - important when missing a positive case is costly (e.g. overlooking a real injury).\n",
    "- ROC AUC: measures how well the model ranks positives higher than negatives - a value close to 1.0 means the model separates classes well; 0.5 means random guessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdec726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.88\n",
      "Precision: 0.00\n",
      "Recall   : 0.00\n",
      "ROC AUC  : 0.38\n",
      "\n",
      "Full classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.93        88\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.88        90\n",
      "   macro avg       0.49      0.45      0.47        90\n",
      "weighted avg       0.95      0.88      0.91        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Recall   : {recall_score(y_test, y_pred):.2f}\")\n",
    "print(f\"ROC AUC  : {roc_auc_score(y_test, y_pred_prob):.2f}\")\n",
    "print(\"\\nFull classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab3c88",
   "metadata": {},
   "source": [
    "# 13. Interpret Model Coefficients\n",
    "\n",
    "Coefficients show how each feature affects injury risk in terms of log-odds.  \n",
    "\n",
    "- Numeric features (e.g., minutes_played, age) indicate the change in injury log-odds per unit increase. Example: A coefficient of 0.07 for 'minutes_played' means that each additional minute slightly increases the risk.\n",
    "- Categorical features (like positions) are interpreted relative to the baseline category ('Defender'). A negative coefficient means a lower injury risk compared to the reference, and a positive one means a higher risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b76e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature impacts on injury risk (log-odds):\n",
      "                      feature  coefficient\n",
      "4              minutes_played     1.081716\n",
      "6                     sprints     0.481941\n",
      "3                         age    -0.103203\n",
      "0            position_Forward    -0.552859\n",
      "7  rest_days_since_last_match    -0.603717\n",
      "5  distance_covered_in_meters    -0.960916\n",
      "2         position_Midfielder    -2.472750\n",
      "1         position_Goalkeeper    -3.043780\n",
      "8   is_back_to_back_away_game    -3.633081\n"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'feature':     X.columns,\n",
    "    'coefficient': model.coef_[0]\n",
    "}).sort_values(by='coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature impacts on injury risk (log-odds):\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37290955",
   "metadata": {},
   "source": [
    "## 14. Interpret the Results\n",
    "\n",
    "Filter only the position coefficients to see the influence of the position on the occurrence of injuries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7169e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Impact of player position on injury risk:\n",
      "               feature  coefficient\n",
      "0     position_Forward    -0.552859\n",
      "2  position_Midfielder    -2.472750\n",
      "1  position_Goalkeeper    -3.043780\n"
     ]
    }
   ],
   "source": [
    "position_coef_df = coef_df[coef_df['feature'].str.startswith('position_')]\n",
    "print(\"\\nImpact of player position on injury risk:\")\n",
    "print(position_coef_df.sort_values(by='coefficient', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
